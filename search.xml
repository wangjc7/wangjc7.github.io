<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[JUnit-4.12使用报java.lang.NoClassDefFoundError: org/hamcrest/SelfDescribing错误]]></title>
    <url>%2F2018%2F09%2F30%2FJUnit-4-12%E4%BD%BF%E7%94%A8%E6%8A%A5java-lang-NoClassDefFoundError-org-hamcrest-SelfDescribing%E9%94%99%E8%AF%AF%2F</url>
    <content type="text"><![CDATA[现象使用最新的JUnit版本，是4.12，结果尝试使用发现总是报java.lang.NoClassDefFoundError: org/hamcrest/SelfDescribing这样的错误。 原因上网查了一下，一般的解决方案是，换一个低一点的版本就好了。还有人说，是缺少hamcrest的包。去官网又看了一下，结果发现这样一段话： junit.jar: Includes the Hamcrest classes. The simple all-in-one solution to get started quickly.Starting with version 4.11, Hamcrest is no longer included in this jar. junit-dep.jar: Only includes the JUnit classes but not Hamcrest. Lets you use a different Hamcrest version. 注意黑色加下划线的部分。说明4.1.2中没有hamcrest包了，不知道作者是怎么想的。 解决方法两种 (1)换成junit-4.8.jar (2)junit-4.12.jar + hamcrest-core-1.3.jar 参考： https://www.cnblogs.com/anny0404/p/5275595.html —end—]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>junit</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[superset安装]]></title>
    <url>%2F2018%2F09%2F18%2Fsuperset%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[supersetSuperset 是 Airbnb 开源的数据分析与可视化平台，同时也是由 Python 语言构建的轻量级 BI 系统。Superset 可实现对 TB 量级数据进行处理，兼容常见的数十种关系或非关系型数据库，并在内部实现 SQL 编辑查询等操作。除此之外，基于 Web 服务的 Superset 可实现多用户协使用，并可针对不同角色进行权限管理。 Install(for linux):1.安装pip 2.安装virtualenv 3.安装mysqlclient,impyla,python-ldap,redis 4.安装superset pip install -i https://pypi.tuna.tsinghua.edu.cn/simple superset==0.22.1 5.修改config.py配置文件，修改元数据存储为Mysql SQLALCHEMY_DATABASE_URI = &apos;mysql://username:password@ip/dbname?charset=utf8&apos; 6.创建管理员用户和密码 fabmanager create-admin --app superset 7.初始化数据 superset db upgrade 8.创建默认角色和权限 superset init 9.启动： superset runserver -p 8388 -d # -d为debug模式启动 10.访问： http://ip:8388 admin/admin superser主页：https://github.com/apache/incubator-superset —end—]]></content>
      <categories>
        <category>superset</category>
      </categories>
      <tags>
        <tag>data analysis</tag>
        <tag>superset</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python_MySql语句插入中内容同时包含单引号和双引号的解决办法]]></title>
    <url>%2F2018%2F09%2F16%2FPython-MySql%E8%AF%AD%E5%8F%A5%E6%8F%92%E5%85%A5%E4%B8%AD%E5%86%85%E5%AE%B9%E5%90%8C%E6%97%B6%E5%8C%85%E5%90%AB%E5%8D%95%E5%BC%95%E5%8F%B7%E5%92%8C%E5%8F%8C%E5%BC%95%E5%8F%B7%E7%9A%84%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95%2F</url>
    <content type="text"><![CDATA[在python中调用MySQLdb模块插入数据信息，假设待输入信息data为：Hello’World”! 其中同时包含了单引号和双引号 一般插入语句为 sql = &quot;insert into tb (my_str) values(&apos;%s&apos;)&quot; % (data) cursor.execute(sql) 其中values(‘%s’)中的%s外面也要有引号，这个引号与data中的引号匹配导致了内容错误 解决办法一: MySQLdb.escape_string() 在MySQLdb模块中自带针对mysql的转义函数escape_string()，直接调用即可 sql = &quot;insert into tb (my_str) values(&apos;%s&apos;)&quot; % (MySQLdb.escape_string(data)) cursor.execute(sql) 解决办法二 cursor.execute() cursor.execute()可以接受一个参数，也可以接受两个参数： cursor.execute(&quot;insert into tablename(c1,c2) values(%s, %s)&quot; , (value1,value2) ); 这种格式是接受两个参数，MySQLdb会自动替你对字符串进行转义和加引号，不必再自己进行转义 实例： #-*-coding: utf8 -*- from connectdb import connectDatabase; #connectDatabase是我自己定义的一个连接数据的函数 import MySQLdb; def escape(): cnn = connectDatabase(); cursor = cnn.cursor(); name = &quot;\\&quot;; name2 = &quot;\&quot;&quot; print name,name2; queryli = [(12,name),(12,name2)] print queryli; #cursor.executemany(&quot;insert into resource(cid,name) values(%s, %s)&quot;,queryli); #接受两个参数，MySQLdb会自动替你对字符串进行转义和加引号，不必再自己进行转义 #执行完此语句之后，resource表中多了一条记录： 12 \ #但如果这么写，cursor.execute(&quot;insert into resource(cid,name) values(%s, %s)&quot; % (12,name) ); #这种格式是利用python的字符串格式化自己生成一个query，也就是传给execute一个参数， #此时必须自己对字符串转义和增加引号，即上边的语句是错误的，应该修改为： #name = MySQLdb.escape_string(name); #cursor.execute(&quot;insert into resource(cid,name) values(%s, &apos;%s&apos;)&quot; % (12,name) ); #这样插入的记录才和(1)一样：12 \ cursor.execute(&quot;insert into resource(cid,name) values(%s, %s)&quot; , (12,name) ); cursor.close(); cnn.commit(); cnn.close(); if __name__ == &quot;__main__&quot;: escape(); 解决办法三 当然也可以自己转义字符 将data变为下面的形式,再插入数据库就正确了Hello\’World\”! 具体在python中的转义函数如下： def transferContent(self, content): if content is None: return None else: string = &quot;&quot; for c in content: if c == &apos;&quot;&apos;: string += &apos;\\\&quot;&apos; elif c == &quot;&apos;&quot;: string += &quot;\\\&apos;&quot; elif c == &quot;\\&quot;: string += &quot;\\\\&quot; else: string += c return string —end—]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>MySQLdb</tag>
        <tag>escape string</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Windows与Linux回车换行符]]></title>
    <url>%2F2018%2F09%2F16%2FWindows%E4%B8%8ELinux%E5%9B%9E%E8%BD%A6%E6%8D%A2%E8%A1%8C%E7%AC%A6%2F</url>
    <content type="text"><![CDATA[回车、换行的区别回车”（Carriage Return）和“换行”（Line Feed） 符号 ASCII码 意义 \n 10 换行 \r 13 回车CR ‘\r’ 回车，回到当前行的行首，而不会换到下一行，如果接着输出的话，本行以前的内容会被逐一覆盖； ‘\n’ 换行，换到当前位置的下一行，而不会回到行首； Unix系统里，每行结尾只有“&lt;换行&gt;”，即”\n”；Windows系统里面，每行结尾是“&lt;回车&gt;&lt;换行&gt;”，即“\r\n”；Mac系统里，每行结尾是“&lt;回车&gt;”，即”\r”；。 一个直接后果是，Unix/Mac系统下的文件在Windows里打开的话，所有文字会变成一行；而Windows里的文件在Unix/Mac下打开的话，在每行的结尾可能会多出一个^M符号。 —end—]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>/r,/n</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python发送邮件]]></title>
    <url>%2F2018%2F09%2F13%2Fpython%E5%8F%91%E9%80%81%E9%82%AE%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[代码： emailhelper.py # -*- coding: UTF-8 -*- import sys import platform import smtplib from email.header import Header from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText # SMTP 服务 mail_host = &quot;smtp.company.com&quot; # 设置服务器 mail_user = &quot;username@company.com&quot; # 用户名 mail_pass = &quot;password&quot; # 口令 def send_mail( receivers, subject, content, filenames=None): &quot;&quot;&quot; :param sender:string :param receivers:list[string] # 如果收件人为多个格式[&apos;xx@a.com&apos;,&apos;yy@b.com&apos;] subject：string content: string filename: string :return: &quot;&quot;&quot; sender=mail_user filenames = [] if filenames is None else filenames #附件名称 try: message = MIMEMultipart() message.attach(MIMEText(content, &apos;plain&apos;, &apos;utf-8&apos;)) message[&apos;Subject&apos;] = Header(subject, &apos;utf-8&apos;) # message[&apos;to&apos;]格式为：xx@aa.com, yy@bb.com message[&apos;to&apos;] = &apos;,&apos;.join(receivers) # 如果附件名称大于0个字符 for filename in filenames: if filename is not None and len(filename.strip()) &gt; 0: att1 = MIMEText(open(filename, &apos;rb&apos;).read(), &apos;base64&apos;, &apos;utf-8&apos;) att1[&quot;Content-Type&quot;] = &apos;application/octet-stream&apos; # 这里的filename可以任意写，写什么名字，邮件中显示什么名字 att1[&quot;Content-Disposition&quot;] = &apos;attachment; filename=&quot;%s&quot; filename message.attach(att1) smtp_obj = smtplib.SMTP() smtp_obj.connect(mail_host, 25) # 25 为 SMTP 端口号 smtp_obj.login(mail_user, mail_pass) smtp_obj.sendmail(sender, receivers, message.as_string()) smtp_obj.quit() print &quot;邮件发送成功&quot; except smtplib.SMTPException, e: print &quot;Error: 无法发送邮件,code:%s&quot; % e.smtp_code return —over—]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>smtplib</tag>
        <tag>email</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive分组取TopN]]></title>
    <url>%2F2018%2F09%2F13%2FHive%E5%88%86%E7%BB%84%E5%8F%96TopN%2F</url>
    <content type="text"><![CDATA[Hive在0.11.0版本开始加入了row_number、rank、dense_rank分析函数，可以查询分组排序后的top值 语法：row_number() over ([partition col1] [order by col2]) rank() over ([partition col1] [order by col2]) dense_rank() over ([partition col1] [order by col2]) 它们都是根据col1字段分组，然后对col2字段进行排序，对排序后的每行生成一个行号，这个行号从1开始递增col1、col2都可以是多个字段，用’,’分隔 区别： row_number：不管col2字段的值是否相等，行号一直递增，比如：有两条记录的值相等，但一个是第一，一个是第二 rank：上下两条记录的col2相等时，记录的行号是一样的，但下一个col2值的行号递增N（N是重复的次数），比如：有两条并列第一，下一个是第三，没有第二 dense_rank：上下两条记录的col2相等时，下一个col2值的行号递增1，比如：有两条并列第一，下一个是第二 row_number可以实现分页查询 实例：hive&gt; create table t(name string, sub string, score int) row format delimited fields terminated by &apos;\t&apos;; 数据在附件的a.txt里 a chinese 98 a english 90 d chinese 88 c english 82 c math 98 b math 89 b chinese 79 z english 90 z math 89 z chinese 80 e math 99 e english 87 d english 90 1. row_number hive (test)&gt; select *, row_number() over (partition by sub order by score) as od from t; 2. rank hive (test)&gt; select *, rank() over (partition by sub order by score) as od from t; 3. dense_ran hive (test)&gt; select *, dense_rank() over (partition by sub order by score desc) from t; 业务实例： 统计每个学科的第一名 select * from (select *, row_number() over (partition by sub order by score desc) as od from t ) t where od=1; 统计每个学科的前三名 select * from (select *, row_number() over (partition by sub order by score desc) as od from t ) t where od&lt;=3; 语文成绩是80分的排名是多少 hive (test)&gt; select od from (select *, row_number() over (partition by sub order by score desc) as od from t ) t where sub=&apos;chinese&apos; and score=80; 分页查询 hive (test)&gt; select * from (select *, row_number() over () as rn from t) t1 where rn between 1 and 5; 原文链接： https://www.cnblogs.com/lishouguang/p/4560837.html —over—]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>row_number rank dense_rank</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CDH集群时钟偏差警报]]></title>
    <url>%2F2018%2F09%2F13%2FCDH%E9%9B%86%E7%BE%A4%E6%97%B6%E9%92%9F%E5%81%8F%E5%B7%AE%E8%AD%A6%E6%8A%A5%2F</url>
    <content type="text"><![CDATA[CDH集群总是提示时钟偏差,但是查看ntpd服务是正常工作的. OS Version: CentOS Linux release 7.5.1804 (Core) CDH Version: CDH 5.8.4 于是google一下原理： The clock NTP health check is executed by each agent running on nodes on your cluster. The command executed is: ntpdc -np A timeout of 2 seconds is used, so if the ntp client does not return in 2 seconds, the health check will fail. If there is a result, then the agent script will parse the result text and return a result metric that includes the clock offset. this will be sent to the Host Monitor Management Service for processing. You have 2 options here: If you are convinced there are no problems, you can turn off the Cloudera Manager Server Clock Offset Thresholds health check or adjust it as necessary in the Cloudera Manager management services. Or, if you wish to troubleshoot, check the /var/log/cloudera-scm-agent/cloudera-scm-agent.log file for clues.Search in that file for “ntpdc”. If there are any errors running the command, a stack trace will be provided. The agent merely parses the ntpdc output, so assuming your output looks something like this: ntpdc -np remote local st poll reach delay offset disp ======================================================================= *132.163.4.101 10.17.81.194 1 1024 377 0.02972 0.001681 0.13664 =198.55.111.5 10.17.81.194 2 1024 377 0.01395 0.002177 0.13667 =50.116.55.65 10.17.81.194 2 1024 377 0.07263 0.001220 0.12172 The script will look for a line that starts with an “*” character. So, in our example: *132.163.4.101 10.17.81.194 1 1024 377 0.02972 0.001681 0.13664 Then, it will get the ‘offset’ column.This value is returned to the Host Monitor which, will pull the metric and filter it through your health check configuration to decide if it warrants an alert. Lastly, I’m not aware that anything has changed in the offset health check between CM 5.3 and 5.4, so I would recommend troubleshooting this to try to figure out why clock is offset. Timing is important in hadoop, so it is worth a look. 参考链接： https://community.cloudera.com/t5/Cloudera-Manager-Installation/Cloudera-5-4-x-cluster-randomly-reports-quot-Clock-Offset-Bad/td-p/31166 —over—]]></content>
      <categories>
        <category>CDH</category>
      </categories>
      <tags>
        <tag>cloudera-scm-agent</tag>
        <tag>ntp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PageRank算法介绍]]></title>
    <url>%2F2018%2F07%2F25%2FPageRank%E7%AE%97%E6%B3%95%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[正文在PageRank发明之前，搜索引擎采用的还是最原始的关键字匹配技术，于是呢在搜索结果中经常会遇到「挂羊头卖狗肉」的垃圾网站，对这些网站，当时的Yahoo采用还是人工清理的方法。 这时候Google的两位创始人Page和Brin就在想，有没有一种算法，能够给出网页重要性的排序呢？这样就可以优先推荐重要网页，而让那些垃圾网页石沉大海了。Page和Brin发现，网页的超链接结构中就蕴含了重要程度的信息。由于一个网页的超链接指向的主要是与其内容相关的网页，那么我们不难想象，如果有许多网页都同时指向某一个网页，这个网页就一定非常重要。我们将互联网想象成一个流网络，网络的节点就是一个个网页，如果两个网页间存在超链接的关系，那么它们之间就存在一条有向的连边。想象存在一种货币，它们在这个流网络上随机地流动，在任意时刻，每个网页上都会有货币流入，也会有货币流出，当最终达到稳定时，将每个网页持有的货币存量，或者说「财富」的多寡由大到小排序，就得到了网页重要性的排序PageRank。 我们发现排在前面的主要是被较多引用的网页，当然有幸被重要网页引用的网页也会得到较大的PageRank值。当然我们还要考虑这样一种情况，如果遇到不引用任何其他网页的「铁公鸡」，或者网页A仅引用B，B仅引用C，C又仅引用A的「小团体」，货币只会流入不会流出，他们会积累大量的货币，但显然他们不一定是最重要的。为避免这样的情况发生，PageRank还使货币可以以概率e跳到系统中的任意其他节点。或者我们可以想象系统中存在一个「中央政府」，它在每一时刻都从各个网页节点的「财富」中征缴比例为e的「税金」，然后再平均分给每一个网页。 链接：https://www.zhihu.com/question/19555545/answer/]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>google</tag>
        <tag>pagerank</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Error found before invoking supervisord: dictionary update sequence element #100 has length 1; 2 is required]]></title>
    <url>%2F2018%2F06%2F04%2FError-found-before-invoking-supervisord-dictionary-update-sequence-element-100-has-length-1-2-is-required%2F</url>
    <content type="text"><![CDATA[现象cloudera manager agent bug report——can’t start tasktracker cloudera manager 4.8 + CDH4.4 + centos5、6agent log below when start TT by CM [10/Feb/2014 09:08:44 +0000] 105652 MainThread util ERROR Failed to parse environment variables: CDH_HCAT_HOME=/usr/lib/hcatalogHADOOP_LOGFILE=hadoop-cmf-mapreduce1-TASKTRACKER-dn3.hadoop.com.log.outTOMCAT_HOME=/usr/lib/bigtop-tomcatHOSTNAME=dn3.hadoop.comHADOOP_SECURITY_LOGGER=INFO,RFASCDH_SOLR_HOME=/usr/lib/solrCDH_PIG_HOME=/usr/lib/pigHADOOP_LOG_DIR=/var/log/hadoop-0.20-mapreduceCLOUDERA_MYSQL_CONNECTOR_JAR=/usr/share/java/mysql-connector-java.jarSHELL=/bin/bashTERM=linuxHISTSIZE=1000SSH_CLIENT=61.135.207.195 50583 22CDH_HUE_PLUGINS_HOME=/usr/lib/hadoopCDH_HIVE_HOME=/usr/lib/hiveQTDIR=/usr/lib64/qt-3.3QTINC=/usr/lib64/qt-3.3/includeSSH_TTY=/dev/pts/2CLOUDERA_ORACLE_CONNECTOR_JAR=/usr/share/java/oracle-connector-java.jarHIVE_DEFAULT_XML=/etc/hive/conf.dist/hive-default.xmlCDH_VERSION=4MGMT_HOME=/usr/share/cmfCMF_CONF_DIR=/etc/cloudera-scm-agentUSER=rootJSVC_HOME=/usr/libexec/bigtop-utilsLS_COLORS=rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=01;05;37;41:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:.tar=01;31:.tgz=01;31:.arj=01;31:.taz=01;31:.lzh=01;31:.lzma=01;31:.tlz=01;31:.txz=01;31:.zip=01;31:.z=01;31:.Z=01;31:.dz=01;31:.gz=01;31:.lz=01;31:.xz=01;31:.bz2=01;31:.tbz=01;31:.tbz2=01;31:.bz=01;31:.tz=01;31:.deb=01;31:.rpm=01;31:.jar=01;31:.rar=01;31:.ace=01;31:.zoo=01;31:.cpio=01;31:.7z=01;31:.rz=01;31:.jpg=01;35:.jpeg=01;35:.gif=01;35:.bmp=01;35:.pbm=01;35:.pgm=01;35:.ppm=01;35:.tga=01;35:.xbm=01;35:.xpm=01;35:.tif=01;35:.tiff=01;35:.png=01;35:.svg=01;35:.svgz=01;35:.mng=01;35:.pcx=01;35:.mov=01;35:.mpg=01;35:.mpeg=01;35:.m2v=01;35:.mkv=01;35:.ogm=01;35:.mp4=01;35:.m4v=01;35:.mp4v=01;35:.vob=01;35:.qt=01;35:.nuv=01;35:.wmv=01;35:.asf=01;35:.rm=01;35:.rmvb=01;35:.flc=01;35:.avi=01;35:.fli=01;35:.flv=01;35:.gl=01;35:.dl=01;35:.xcf=01;35:.xwd=01;35:.yuv=01;35:.cgm=01;35:.emf=01;35:.axv=01;35:.anx=01;35:.ogv=01;35:.ogx=01;35:.aac=01;36:.au=01;36:.flac=01;36:.mid=01;36:.midi=01;36:.mka=01;36:.mp3=01;36:.mpc=01;36:.ogg=01;36:.ra=01;36:.wav=01;36:.axa=01;36:.oga=01;36:.spx=01;36:.xspf=01;36:SSH_AUTH_SOCK=/tmp/ssh-REFP104132/agent.104132CDH_IMPALA_HOME=/usr/lib/impalaCDH_HTTPFS_HOME=/usr/lib/hadoop-httpfsCDH_MR2_HOME=/usr/lib/hadoop-mapreduceCDH_MR1_HOME=/usr/lib/hadoop-0.20-mapreduceCDH_HUE_HOME=/usr/share/hueCM_STATUS_CODES=STATUS_NONE HDFS_DFS_DIR_NOT_EMPTY JOBTRACKER_IN_STANDBY_MODEMAIL=/var/spool/mail/rootPATH=/sbin:/usr/sbin:/bin:/usr/bin:/usr/kerberos/binCONF_DIR=/var/run/cloudera-scm-agent/process/364-mapreduce-TASKTRACKERPWD=/var/log/cloudera-scm-agentLANG=zh_CN.UTF-8KDE_IS_PRELINKED=1CDH_HADOOP_HOME=/usr/lib/hadoopMODULEPATH=/usr/share/Modules/modulefiles:/etc/modulefilesLOADEDMODULES=KDEDIRS=/usrHADOOP_AUDIT_LOGGER=INFO,RFAAUDITCDH_SQOOP2_HOME=/usr/lib/sqoop2HISTCONTROL=ignoredupsKRB5CCNAME=/tmp/krb5cc_cm_agentCDH_HDFS_HOME=/usr/lib/hadoop-hdfsCDH_HADOOP_BIN=/usr/bin/hadoopSSH_ASKPASS=/usr/libexec/openssh/gnome-ssh-askpassSHLVL=3HOME=/rootCDH_OOZIE_HOME=/usr/lib/oozieCDH_HBASE_INDEXER_HOME=/usr/lib/hbase-solrCDH_FLUME_HOME=/usr/lib/flume-ngLOGNAME=rootQTLIB=/usr/lib64/qt-3.3/libCVS_RSH=sshSSH_CONNECTION=61.135.207.195 50583 223.202.52.31 22CDH_HBASE_HOME=/usr/lib/hbaseCDH_ZOOKEEPER_HOME=/usr/lib/zookeeperMODULESHOME=/usr/share/ModulesCDH_YARN_HOME=/usr/lib/hadoop-yarnLESSOPEN=|/usr/bin/lesspipe.sh %sHADOOP_TASKTRACKER_OPTS=-Xms1073741824 -Xmx1073741824 -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:-CMSConcurrentMTEnabled -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled -Dhadoop.event.appender=,EventCatcher -Dhadoop.security.logger=INFO,RFASHADOOP_ROOT_LOGGER=INFO,RFACMF_PACKAGE_DIR=/usr/lib64/cmf/serviceCLOUDERA_POSTGRESQL_JDBC_JAR=/usr/share/cmf/lib/postgresql-9.0-801.jdbc4.jarG_BROKEN_FILENAMES=1HISTTIMEFORMAT=%F %Tmodule=() { eval `/usr/bin/modulecmd bash $*`}_=/bin/envTraceback (most recent call last):File “/usr/lib64/cmf/agent/src/cmf/util.py”, line 286, in sourcereturn dict((line.split(“=”, 1) for line in data.splitlines()))ValueError: dictionary update sequence element #71 has length 1; 2 is required[10/Feb/2014 09:08:44 +0000] 105652 MainThread agent ERROR Failed to activate {u’status_links’: {u’status’: u’http://dn3.hadoop.com:50060/&#39;}, u’name’: u’mapreduce-TASKTRACKER’, u’config_generation’: 5, u’configuration_data’: ‘PK …(省略) u’refresh_files’: [], u’user’: u’mapred’, u’auto_restart’: True, u’run_generation’: 1, u’parcels’: {}, u’environment’: {u’HADOOP_AUDIT_LOGGER’: u’INFO,RFAAUDIT’, u’HADOOP_TASKTRACKER_OPTS’: u’-Xms1073741824 -Xmx1073741824 -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:-CMSConcurrentMTEnabled -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled -Dhadoop.event.appender=,EventCatcher -Dhadoop.security.logger=INFO,RFAS’, u’HADOOP_SECURITY_LOGGER’: u’INFO,RFAS’, u’HADOOP_LOG_DIR’: u’/var/log/hadoop-0.20-mapreduce’, u’HADOOP_ROOT_LOGGER’: u’INFO,RFA’, u’HADOOP_LOGFILE’: u’hadoop-cmf-mapreduce1-TASKTRACKER-dn3.hadoop.com.log.out’, u’CDH_VERSION’: u’4’}, u’optional_tags’: [u’cdh-plugin’, u’mapreduce-plugin’], u’running’: True, u’program’: u’mapreduce/mapreduce.sh’, u’required_tags’: [u’cdh’], u’arguments’: [u’tasktracker’], u’special_file_info’: [{u’permissions’: 420, u’deploy_dir’: u’$CDH_MR1_HOME/conf’, u’group’: u’root’, u’name’: u’taskcontroller.cfg’, u’user’: u’root’}], u’group’: u’hadoop’, u’id’: 364, u’resources’: [{u’file’: None, u’tcp_listen’: None, u’dynamic’: True, u’io’: None, u’memory’: None, u’directory’: None, u’cpu’: {u’shares’: 1024}}, {u’file’: None, u’tcp_listen’: None, u’dynamic’: True, u’io’: {u’weight’: 500}, u’memory’: None, u’directory’: None, u’cpu’: None}, {u’file’: None, u’tcp_listen’: None, u’dynamic’: False, u’io’: None, u’memory’: {u’hard_limit’: -1, u’soft_limit’: -1}, u’directory’: None, u’cpu’: None}, {u’file’: None, u’tcp_listen’: None, u’dynamic’: False, u’io’: None, u’memory’: None, u’directory’: {u’path’: u’/var/log/hadoop-0.20-mapreduce’, u’bytes_free_warning_threshhold_bytes’: 0, u’group’: u’hadoop’, u’user’: u’root’, u’mode’: 509}, u’cpu’: None}, {u’file’: None, u’tcp_listen’: None, u’dynamic’: False, u’io’: None, u’memory’: None, u’directory’: {u’path’: u’/var/lib/hadoop-mapreduce’, u’bytes_free_warning_threshhold_bytes’: 0, u’group’: u’hadoop’, u’user’: u’mapred’, u’mode’: 493}, u’cpu’: None}, {u’file’: None, u’tcp_listen’: None, u’dynamic’: False, u’io’: None, u’memory’: None, u’directory’: {u’path’: u’/data2/mapred/local’, u’bytes_free_warning_threshhold_bytes’: 0, u’group’: u’hadoop’, u’user’: u’mapred’, u’mode’: 493}, u’cpu’: None}, {u’file’: None, u’tcp_listen’: None, u’dynamic’: False, u’io’: None, u’memory’: None, u’directory’: {u’path’: u’/data3/mapred/local’, u’bytes_free_warning_threshhold_bytes’: 0, u’group’: u’hadoop’, u’user’: u’mapred’, u’mode’: 493}, u’cpu’: None}, {u’file’: None, u’tcp_listen’: None, u’dynamic’: False, u’io’: None, u’memory’: None, u’directory’: {u’path’: u’/data4/mapred/local’, u’bytes_free_warning_threshhold_bytes’: 0, u’group’: u’hadoop’, u’user’: u’mapred’, u’mode’: 493}, u’cpu’: None}, {u’file’: None, u’tcp_listen’: None, u’dynamic’: False, u’io’: None, u’memory’: None, u’directory’: {u’path’: u’/data5/mapred/local’, u’bytes_free_warning_threshhold_bytes’: 0, u’group’: u’hadoop’, u’user’: u’mapred’, u’mode’: 493}, u’cpu’: None}, {u’file’: None, u’tcp_listen’: None, u’dynamic’: False, u’io’: None, u’memory’: None, u’directory’: {u’path’: u’/data6/mapred/local’, u’bytes_free_warning_threshhold_bytes’: 0, u’group’: u’hadoop’, u’user’: u’mapred’, u’mode’: 493}, u’cpu’: None}, {u’file’: None, u’tcp_listen’: {u’bind_address’: u’172.16.11.187’, u’port’: 50060}, u’dynamic’: False, u’io’: None, u’memory’: None, u’directory’: None, u’cpu’: None}], u’one_off’: False}Traceback (most recent call last):File “/usr/lib64/cmf/agent/src/cmf/agent.py”, line 1004, in handle_heartbeat_responsenew_process.activate()File “/usr/lib64/cmf/agent/src/cmf/agent.py”, line 1899, in activateself.write_process_conf()File “/usr/lib64/cmf/agent/src/cmf/agent.py”, line 1990, in write_process_conf“source_parcel_environment”, env))File “/usr/lib64/cmf/agent/src/cmf/util.py”, line 289, in sourceraise eValueError: dictionary update sequence element #71 has length 1; 2 is required ========================== when i run linux command ‘env’ ,i saw some line has no symbol ‘=’： #env …module=() { eval `/usr/bin/modulecmd bash $*`} &lt;&lt;&lt;== look here_=/bin/env according to error log, i open agent file “/usr/lib64/cmf/agent/src/cmf/util.py “, line 271-289: 271 def source(path, command, caller_env):272 “””273 Source a shell script in a subprocess and extract its environment and return274 it as a dict. For our specific purposes we need to support executing a command275 after sourcing the script to correctly populate the environment.276 “””277 for k, v in caller_env.iteritems():278 if v is None:279 LOG.warn(“None environment value: %s=%s” % (k, v,))280281 pipe = subprocess.Popen([‘/bin/bash’, ‘-c’, “. %s; %s; env | grep =” % (path, command)],282 stdout=subprocess.PIPE, env=caller_env)283 data = pipe.communicate()[0]284285 try:286 return dict((line.split(“=”, 1) for line in data.splitlines())) &lt;&lt;&lt;&lt;&lt;=== look here287 except Exception, e:288 LOG.exception(“Failed to parse environment variables: “ + data)289 raise e line 286 split line by “=”, then add to dict. when the line not contans ‘=’, error would throws. 解决fix it: change281 pipe = subprocess.Popen([‘/bin/bash’, ‘-c’, “. %s; %s; env” % (path, command)], to281 pipe = subprocess.Popen([‘/bin/bash’, ‘-c’, “. %s; %s; env | grep =” % (path, command)], then:/etc/init.d/cloudera-scm-agent restartopen cloudera manager, start TT The most likely reason this is occurring is because the Cloudera Manager agent was started by invoking the init.d script rather than using the service command (BAD) /etc/init.d/cloudera-scm-agent start(GOOD)service cloudera-scm-agent start 参考http://community.cloudera.com/t5/Cloudera-Manager-Installation/Can-t-start-the-tasktracker/td-p/20596]]></content>
      <categories>
        <category>CDH</category>
      </categories>
      <tags>
        <tag>nodemanager</tag>
        <tag>cdh</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive UDF整理]]></title>
    <url>%2F2018%2F05%2F25%2Fhive-UDF%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[[转]Hive UDF整理字符串函数##字符串长度函数：length语法: length(string A) 返回值: int 说明：返回字符串A的长度 举例： hive&gt; select length(‘abcedfg’) from dual; 7 ##字符串反转函数：reverse语法: reverse(string A) 返回值: string 说明：返回字符串A的反转结果 举例： hive&gt; select reverse(‘abcedfg’) from dual; gfdecba ##字符串连接函数：concat语法: concat(string A, string B…) 返回值: string 说明：返回输入字符串连接后的结果，支持任意个输入字符串 举例： hive&gt; select concat(‘abc’,&apos;def’,&apos;gh’) from dual; abcdefgh ##带分隔符字符串连接函数：concat_ws语法: concat_ws(string SEP, string A, string B…) 返回值: string 说明：返回输入字符串连接后的结果，SEP表示各个字符串间的分隔符 举例： hive&gt; select concat_ws(‘,’,&apos;abc’,&apos;def’,&apos;gh’) from dual; abc,def,gh ##字符串截取函数：substr,substring语法: substr(string A, int start),substring(string A, int start) 返回值: string 说明：返回字符串A从start位置到结尾的字符串 举例： hive&gt; select substr(‘abcde’,3) from dual; cde hive&gt; select substring(‘abcde’,3) from dual; cde hive&gt; select substr(‘abcde’,-1) from dual; （和ORACLE相同） e ##字符串截取函数：substr,substring语法: substr(string A, int start, int len),substring(string A, int start, int len) 返回值: string 说明：返回字符串A从start位置开始，长度为len的字符串 举例： hive&gt; select substr(‘abcde’,3,2) from dual; cd hive&gt; select substring(‘abcde’,3,2) from dual; cd hive&gt;select substring(‘abcde’,-2,2) from dual; de ##字符串转大写函数：upper,ucase语法: upper(string A) ucase(string A) 返回值: string 说明：返回字符串A的大写格式 举例： hive&gt; select upper(‘abSEd’) from dual; ABSED hive&gt; select ucase(‘abSEd’) from dual; ABSED ##字符串转小写函数：lower,lcase语法: lower(string A) lcase(string A) 返回值: string 说明：返回字符串A的小写格式 举例： hive&gt; select lower(‘abSEd’) from dual; absed hive&gt; select lcase(‘abSEd’) from dual; absed ##去空格函数：trim语法: trim(string A) 返回值: string 说明：去除字符串两边的空格 举例： hive&gt; select trim(‘ abc ‘) from dual; abc ##左边去空格函数：ltrim语法: ltrim(string A) 返回值: string 说明：去除字符串左边的空格 举例： hive&gt; select ltrim(‘ abc ‘) from dual; abc ##右边去空格函数：rtrim语法: rtrim(string A) 返回值: string 说明：去除字符串右边的空格 举例： hive&gt; select rtrim(‘ abc ‘) from dual; abc ##正则表达式替换函数：regexp_replace语法: regexp_replace(string A, string B, string C) 返回值: string 说明：将字符串A中的符合java正则表达式B的部分替换为C。注意，在有些情况下要使用转义字符 举例： hive&gt; select regexp_replace(‘foobar’, ‘oo|ar’, ”) from dual; fb ##正则表达式解析函数：regexp_extract语法: regexp_extract(string subject, string pattern, int index) 返回值: string 说明：将字符串subject按照pattern正则表达式的规则拆分，返回index指定的字符。注意，在有些情况下要使用转义字符 举例： hive&gt; select regexp_extract(‘foothebar’, ‘foo(.*?)(bar)’, 1) from dual; the hive&gt; select regexp_extract(‘foothebar’, ‘foo(.*?)(bar)’, 2) from dual; bar hive&gt; select regexp_extract(‘foothebar’, ‘foo(.*?)(bar)’, 0) from dual; foothebar ##URL解析函数：parse_url语法: parse_url(string urlString, string partToExtract [, string keyToExtract]) 返回值: string 说明：返回URL中指定的部分。partToExtract的有效值为：HOST, PATH, QUERY, REF, PROTOCOL, AUTHORITY, FILE, and USERINFO. 举例： hive&gt; select parse_url(‘http://facebook.com/path1/p.php?k1=v1&amp;k2=v2#Ref1′, ‘HOST’) from dual; facebook.com hive&gt; select parse_url(‘http://facebook.com/path1/p.php?k1=v1&amp;k2=v2#Ref1′, ‘QUERY’, ‘k1′) from dual; v1 ##json解析函数：get_json_object语法: get_json_object(string json_string, string path) 返回值: string 说明：解析json的字符串json_string,返回path指定的内容。如果输入的json字符串无效，那么返回NULL。 举例： hive&gt; select get_json_object(&apos;{&quot;store&quot;:{&quot;fruit&quot;:[{&quot;weight&quot;:8,&quot;type&quot;:&quot;apple&quot;},{&quot;weight&quot;:9,&quot;type&quot;:&quot;pear&quot;}],&quot;bicycle&quot;:{&quot;price&quot;:19.95,&quot;color&quot;:&quot;red&quot;}},&quot;email&quot;:&quot;amy@only_for_json_udf_test.net&quot;,&quot;owner&quot;:&quot;amy&quot;}&apos;,&apos;$.owner&apos;) from dual; amy ##空格字符串函数：space语法: space(int n) 返回值: string 说明：返回长度为n的字符串 举例： hive&gt; select space(10) from dual; hive&gt; select length(space(10)) from dual; 10 ##重复字符串函数：repeat语法: repeat(string str, int n) 返回值: string 说明：返回重复n次后的str字符串 举例： hive&gt; select repeat(‘abc’,5) from dual; abcabcabcabcabc ##首字符ascii函数：ascii语法: ascii(string str) 返回值: int 说明：返回字符串str第一个字符的ascii码 举例： hive&gt; select ascii(‘abcde’) from dual; 97 ##左补足函数：lpad语法: lpad(string str, int len, string pad) 返回值: string 说明：将str进行用pad进行左补足到len位 举例： hive&gt; select lpad(‘abc’,10,’td’) from dual; tdtdtdtabc 与GP，ORACLE不同，pad 不能默认 ##右补足函数：rpad语法: rpad(string str, int len, string pad) 返回值: string 说明：将str进行用pad进行右补足到len位 举例： hive&gt; select rpad(‘abc’,10,’td’) from dual; abctdtdtdt ##分割字符串函数: split语法: split(string str, string pat) 返回值: array 说明: 按照pat字符串分割str，会返回分割后的字符串数组 举例： hive&gt; select split(‘abtcdtef’,&apos;t’) from dual; [&quot;ab&quot;,&quot;cd&quot;,&quot;ef&quot;] ##集合查找函数: find_in_set语法: find_in_set(string str, string strList) 返回值: int 说明: 返回str在strlist第一次出现的位置，strlist是用逗号分割的字符串。如果没有找该str字符，则返回0（只能是逗号分隔，不然返回0） 举例： hive&gt; select find_in_set(‘ab’,&apos;ef,ab,de’) from dual; 2 hive&gt; select find_in_set(‘at’,&apos;ef,ab,de’) from dual; 0 原文地址： https://my.oschina.net/repine/blog/193867 http://www.oratea.net/?p=951]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>udf</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python正则表达式匹配反斜杠]]></title>
    <url>%2F2018%2F05%2F19%2FPython%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%8C%B9%E9%85%8D%E5%8F%8D%E6%96%9C%E6%9D%A0%2F</url>
    <content type="text"><![CDATA[Python正则式的过程中，有一个问题一直困扰我，如何去匹配一个反斜杠（即”\”）？ 一、普通字符串转义反斜杠，在Python中比较特殊，就是它可以用来构成一些特殊字符，比如“\n”表示换行，“\t”表示制表符。下面是使用“\n”的一行代码： 1print 'Hello\World\nPython' 结果为：“Hello\WorldPython“ 可以看到其中的“\n”已转义为换行符，而“\W”没有发生转义，原因是“\W”在“字符串转义”中并不对应着特殊字符，没有特殊含义。 如果现在要求变了，要求不对“\n”转义为换行，而是原封不动输出为“Hello\World\nPython”，该怎么办呢？ 1）可以这样写“Hello\World\\nPython”，这样输出的时候，“字符串转义”会把“\\”转义为“\”； 2）也可使用另一种方法：原始字符串；原始字符串（即r’…’）：字符串中所有字符都直接按照字面意思来使用，不转义特殊字符。 下面是使用原始字符串的代码： 1print r'Hello\World\nPython' 结果为：“Hello\World\nPython”可以清楚看到，在使用原始字符串之后，“\n”未被转义为换行符，而是直接被输出了。 二、正则转义好了，上面讲的只是“字符串转义”。同理，在正则表达式中也存在转义，我们姑且先称其为“正则转义”，其与“字符串转义”完全不同，比如“\d”代表数字，“\s”代表空白符。下面我们先编写开头的例子，然后再分析。 提取“3\8”反斜杠之前的数字： 12345678910111213141516#!/usr/bin/env python #coding=utf-8 import re string = '3\8' m = re.search('(\d+)\\\\', string) if m is not None: print m.group(1) # 结果为：3 n = re.search(r'(\d+)\\', string) if n is not None: print n.group(1) # 结果为：3 正则表达式字符串需要经过两次转义，这两次分别是上面的“字符串转义”和“正则转义”，个人认为“字符串转义”一定先于“正则转义”。 1）’\\‘的过程：先进行“字符串转义”，前两个反斜杠和后两个反斜杠分别被转义成了一个反斜杠；即“\|\”被转成了“|\”（“|”为方便看清，请自动忽略）。“字符串转义”后马上进行“正则转义”，“\”被转义为了“\”，表示该正则式需要匹配一个反斜杠。 2）r’\‘的过程：由于原始字符串中所有字符直接按照字面意思来使用，不转义特殊字符，故不做“字符串转义”，直接进入第二步“正则转义”，在正则转义中“\”被转义为了“\”，表示该正则式需要匹配一个反斜杠。 三、结论也就是说原始字符串（即r’…’）与“正则转义”毫无关系，原始字符串仅在“字符串转义”中起作用，使字符串免去一次转义。 也许有哥们会问，为什么“\d+\\”中的“\d+”即使没用原始字符串，也没出现什么问题。那是因为在做“字符串转义”时，“\d”并不对应特殊字符，所以顺利的留到了“正则转义”时再处理，在“正则转义”中其表示数字。 参考自《Python核心编程》第二版，如有不恰当的地方，还望包容和指出，感谢。 转自：https://blog.csdn.net/jinixin/article/details/56705284]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>regx</tag>
        <tag>escape</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Impala优化基本方案]]></title>
    <url>%2F2018%2F05%2F17%2FImpala%E4%BC%98%E5%8C%96%E5%9F%BA%E6%9C%AC%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[理论本文源自cloudera官网上的Impala文档，原名为《Impala Performance Guidelines and Best Practices》。主要介绍了为了提升impala性能应该考虑的一些事情，这些条目算是对于性能提升最基本的约束了，条目分别如下： 选择合适的文件存储格式，既然使用impala，无非就是为了一个目的：性能好/资源消耗少，Impala为了做到通用性，也就是为了更好的hive无缝连接，支持了大部分Hive支持的文件格式，例如Text、Avro、RCFile、Parquet等（不支持ORC），但是为了实现更快的ad-hoc查询（基本上都是OLAP查询，查询部分列，聚合，分析），我们基本上都会选择使用Parquet格式作为数据文件存储格式，即使你的数据导入到hive中存储的使用的是其它格式（甚至通过自定义serde解析，例如Json），仍然建议你新建一个Parquet格式的表，然后进行一次数据的转换。因此这个条目可以看做是：请选用Parquet作为文件存储格式！ 选择合适的Partition粒度，分区的个数通常是根据业务数据来的，通常时间分区（例如日期/月份）是少不了的，例如对于一个支持多终端的应用，可能在时间分区下面再加一层终端类型的分区，设置对于每一个终端的不同操作在进行一层分区，根据唯物辩证法，凡事都需要保持一个度，那么就从两个极端的情况下来分析分区的粒度如何确定：1：分区过少:，整个表不使用分区，或者只有一个日期的分区，这样会导致频繁的查询某一个终端的数据不得不扫描整天的数据甚至整个表的数据，这是一种浪费；2、分区过多，对于每一个要统计的维度都创建一个分区，这样对于任何一个维度=’xxx’的查询都只需要扫描精确需要的数据，但是这样会导致大量的数据目录，进而导致大量的文件需要扫描，这对于查询优化器是一个灾难。因此最终的建议是：根据查询需求确定分区的粒度，根据每一个分区的成员个数预估总的分区数，保证一个表的分区数不超过30000（经验之谈？），避免过小的分区。 尽量分区的成员的长度，目前分区字段可以支持数值类型和字符串，但是这里推荐尽可能的使用合适的整数（一般用0-256就可以保存一个分区成员的映射了，否则分区会很多）而非原始的字符串，可以在外面建立字符串到整数的映射以保存原始信息，这个约束的主要原因是每一个分区会占用一个目录，每一个目录名又会在NameNode中占用一定的内存，所以不光光是对于Impala而言，对于使用Hadoop的用户而言，尽量减小文件目录的长度。 选择合适的Parquet Block大小，在条目1中已经明确，要使用Impala获得较快的查询性能，那么就老老实实的使用Parquet作为存储格式，而每一个Parquet的Block大小又有什么影响呢，这里暂且把Block的大小理解成一个Parquet分区的大小，在存储上表现为文件大小，如果文件过大，那么会导致这个文件只会一个Impalad进程处理，这样大大降低了Impala的并行处理能力；而如果文件过小则会导致大量的小文件，在带来并发执行的同时也会带来大量的随机I/O的影响，因此需要对于特定的数据进行不同的parquet Block大小测试以寻求最适合该数据集的Block大小。 收集表和分区的统计信息，在执行完数据导入之后，建议使用 COMPUTE STATS语句收集表的统计信息，当然也可以只收集某一个分区的统计信息。 减少返回结果大小，如果需要统计聚合，直接在SQL中完成，尽可能的在where中执行过滤而不要查出来之后在应用端做过滤，对于查询结果尽可能使用LIMIT限制返回结果集大小；避免大量的结果展示在终端，可以考虑通过INSERT xxx的方式把结果输出到文件，或者通过impala-shell参数将结果重定向。 对于执行性能较差的查询使用EXPLAIN分析原因。 最后，查询操作系统配置、查看系统使用负载，可以使用Query Profile工具来探测。 上面的这些条目是最基本的应对性能调优的方案，主要包括：使用Parquet格式存储数据、分区粒度要确定好，保证整个表的分区数不要太多（目录不要太多），每一个分区下不要存在过多的小文件（选择合适的Parquet文件大小），收集统计信息使得查询优化器能够选择更好的查询方案，最后要学会使用EXPLAIN和Profile功能分析性能问题所在。 实例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788[hadoop15:21000] &gt; explain "此处sql省略"，原SQL执行时间7'+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Explain String |+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Estimated Per-Host Requirements: Memory=432.00MB VCores=3 || WARNING: The following tables are missing relevant table and/or column statistics. || db1.tab1, db2.tab2 || || 26:EXCHANGE [UNPARTITIONED] || | || 14:HASH JOIN [LEFT OUTER JOIN, PARTITIONED] || | || | || |--25:EXCHANGE [HASH(to_date(...))] || | | || | 12:AGGREGATE [FINALIZE] || | | output: ... || | | group by: ... || | | || | 24:AGGREGATE || | | group by: ... || | | || | 23:EXCHANGE [HASH(...)] || | | || | 11:AGGREGATE [STREAMING] || | | group by: ... || | | || | 10:HASH JOIN [LEFT OUTER JOIN, PARTITIONED] || | | hash predicates: ... || | | other predicates: ... | | | || | |--22:EXCHANGE [HASH(...)] || | | | || | | 09:SCAN HDFS [...] || | | partitions=1/1 files=4 size=572.48MB || | | predicates: ... || | | || | 21:EXCHANGE [HASH(...)] || | | || | 08:SCAN HDFS [...] || | partitions=2360/2360 files=2360 size=1.84GB || | predicates: ... || | || 13:HASH JOIN [RIGHT OUTER JOIN, PARTITIONED] || | hash predicates: ... || | || |--02:AGGREGATE [FINALIZE] || | | output: ... || | | group by: ... || | | || | 20:AGGREGATE || | | group by: ... || | | || | 19:EXCHANGE [HASH(...)] || | | || | 01:AGGREGATE [STREAMING] || | | group by: ... || | | || | 00:SCAN HDFS [warehouse.mar_house] || | partitions=1/1 files=4 size=572.48MB || | predicates: ... || | || 07:AGGREGATE [FINALIZE] | || 18:AGGREGATE || | group by: ... || | || 17:EXCHANGE [HASH(...)] || | || 06:AGGREGATE [STREAMING] || | group by: ... || | || 05:HASH JOIN [LEFT OUTER JOIN, PARTITIONED] || | hash predicates: ... || | other predicates: ... || | || |--16:EXCHANGE [HASH(...)] || | | || | 04:SCAN HDFS [warehouse.mar_house mh] || | partitions=1/1 files=4 size=572.48MB || | predicates: ... || | || 15:EXCHANGE [HASH(...)] || | || 03:SCAN HDFS [...] || partitions=2360/2360 files=2360 size=1.84GB || predicates: ... |+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+Fetched 85 row(s) in 0.26s 备注：由于sql为生产代码，不方便公开，仅适用示意代码，不影响表达主题注意这行： WARNING: The following tables are missing relevant table and/or column statistics. db1.tab1, db2.tab2 因此考虑对这两张表收集统计信息 123456789101112131415161718192021[hadoop15:21000] &gt; compute stats db1.tab1;Query: compute stats db1.tab1+-------------------------------------------+| summary |+-------------------------------------------+| Updated 1 partition(s) and 100 column(s). |+-------------------------------------------+Fetched 1 row(s) in 4.87s[hadoop15:21000] &gt; compute stats db2.tab2;Query: compute stats db2.tab2+---------------------------------------------+| summary |+---------------------------------------------+| Updated 2360 partition(s) and 96 column(s). |+---------------------------------------------+Fetched 1 row(s) in 148.34s[hadoop15:21000] &gt; [hadoop15:21000] &gt; 再次执行SQL+------------+----------+-------------+-----------+-------------+----------结果集+------------+----------+-------------+-----------+-------------+-----------+Fetched 10 row(s) in 0.94s]]></content>
      <categories>
        <category>impala</category>
      </categories>
      <tags>
        <tag>impala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mapReduce job hangs]]></title>
    <url>%2F2018%2F05%2F16%2FmapReduce-job-hangs%2F</url>
    <content type="text"><![CDATA[现象I tried to run simple word count as MapReduce job，but mapReduce job hangs, waiting for AM container to be allocated. Output from job: *** START *** 15/12/25 17:52:50 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032 15/12/25 17:52:51 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this. 15/12/25 17:52:51 INFO input.FileInputFormat: Total input paths to process : 5 15/12/25 17:52:52 INFO mapreduce.JobSubmitter: number of splits:5 15/12/25 17:52:52 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1451083949804_0001 15/12/25 17:52:53 INFO impl.YarnClientImpl: Submitted application application_1451083949804_0001 15/12/25 17:52:53 INFO mapreduce.Job: The url to track the job: http://hadoop-droplet:8088/proxy/application_1451083949804_0001/ 15/12/25 17:52:53 INFO mapreduce.Job: Running job: job_1451083949804_0001 YarnApplicationState:ACCEPTED: waiting for AM container to be allocated,launched and register with RM. 诊断You should check the status of Node managers in your cluster. If the NM nodes are short on disk space then RM will mark them “unhealthy” and those NMs can’t allocate new containers. 1) Check the Unhealthy nodes: http://&lt;active_RM&gt;:8088/cluster/nodes/unhealthy If the “health report” tab says “local-dirs are bad” then it means you need to cleanup some disk space from these nodes. 2) Check the DFS dfs.data.dir property in hdfs-site.xml. It points the location on local file system where hdfs data is stored. 3) Login to those machines and use df -h &amp; hadoop fs - du -h commands to measure the space occupied. 4) Verify hadoop trash and delete it if it’s blocking you. hadoop fs -du -h /user/user_name/.Trash and hadoop fs -rm -r /user/user_name/.Trash/* 访问：http://yarn001:8088/cluster/nodes/unhealthy 发现符合第一种情况，Healthy报告如下： 1/1 local-dirs are bad: /tmp/hadoop-wang/nm-local-dir; 1/1 log-dirs are bad: /Users/wang/soft/hadoop-2.7.5/logs/userlogs 关于该问题网上说最常见原因是由于节点上的磁盘使用率超出了max-disk-utilization-per-disk-percentage（默认为90.0%）。wangdeMacBook-Pro:hadoop-2.7.5 wang$ df -h Filesystem Size Used Avail Capacity iused ifree %iused Mounted on /dev/disk1 465Gi 446Gi 18Gi 97% 117017228 4822386 96% / 磁盘使用率已经是79%，确实是这个原因，但是这个可能只是表面原因。可以通过增加磁盘利用率阈值来缓解问题：在yarn-site.xml中填写： &lt;property&gt; &lt;name&gt;yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage&lt;/name&gt; &lt;value&gt;98.5&lt;/value&gt; &lt;/property&gt; 但是增加阈值的方法治标不治本，迟早有一天会超过你设置的98.5%。建议清理磁盘空间或扩容。over.]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>mapreduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[单链表反转(Java)]]></title>
    <url>%2F2018%2F05%2F15%2F%E5%8D%95%E9%93%BE%E8%A1%A8%E5%8F%8D%E8%BD%AC-Java%2F</url>
    <content type="text"><![CDATA[单链表反转(Java)原理单链表反转使用p、q、r三个指针配合工作，使得两个节点间的指向反向，同时用r记录剩下的链表。基本流程如下图所示： 代码实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class ReverseList &#123; public static Node reverseList(Node head) &#123; Node p = new Node(0); Node q = new Node(0); Node r = new Node(0); p = head; q = head.next; p.next = null; while (q != null) &#123; r = q.next; q.next = p; p = q; q = r; &#125; head = p; return head; &#125; public static void main(String[] args) &#123; int count = 9; Node t = new Node(1); Node x = t; for (int i = 2; i &lt;= count; i++) &#123; x = (x.next = new Node(i)); &#125; t = reverseList(t); while (t != null) &#123; System.out.print(t.val + " "); t = t.next; &#125; &#125; public static class Node &#123; int val; Node next; Node(int v) &#123; val = v; &#125; &#125;&#125; 转载：http://www.jianshu.com/p/5043be2fc875]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>singly linked list</tag>
        <tag>java</tag>
      </tags>
  </entry>
</search>
